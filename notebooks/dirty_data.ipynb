{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script python for make dirty data and remove rows randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function for make data dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dirty(df, missing_value_ratio=0.06, outlier_ratio=0.05):\n",
    "    \"\"\"\n",
    "    A function to make the data 'dirty' by adding missing values (NaN)\n",
    "    randomly (excluding ID columns) and adding outliers to some numeric columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be made dirty.\n",
    "        missing_value_ratio (float): The proportion of values to be changed to NaN (0.0 - 1.0).\n",
    "        outlier_ratio (float): The proportion of values to be changed to outliers (0.0 - 1.0).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dirty DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    num_rows, num_cols = df_copy.shape\n",
    "\n",
    "    # Kolom-kolom ID yang tidak boleh diubah jadi NaN\n",
    "    id_columns = [\n",
    "        \"customer_id\",\n",
    "        \"customer_unique_id\",\n",
    "        \"order_id\",\n",
    "        \"product_id\",\n",
    "        \"seller_id\",\n",
    "        \"review_id\",\n",
    "    ]\n",
    "\n",
    "    # 1. Add missing values (NaN) randomly, excluding ID columns\n",
    "    non_id_columns = [col for col in df_copy.columns if col not in id_columns]\n",
    "    num_non_id_cols = len(non_id_columns)\n",
    "    if num_non_id_cols > 0:\n",
    "        num_missing = int(missing_value_ratio * num_rows * num_non_id_cols)\n",
    "        cells_to_null = random.sample(range(num_rows * num_non_id_cols), num_missing)\n",
    "\n",
    "        for idx in cells_to_null:\n",
    "            row_idx = idx // num_non_id_cols\n",
    "            col_name = non_id_columns[idx % num_non_id_cols]\n",
    "            df_copy.loc[row_idx, col_name] = np.nan\n",
    "\n",
    "        print(\n",
    "            f\"Number of missing values (NaN) added (excluding ID columns): {num_missing}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No non-ID columns found to add missing values.\")\n",
    "\n",
    "    # 2. Add outliers to some numeric columns (manually selected)\n",
    "    numeric_cols = df_copy.select_dtypes(include=np.number).columns.tolist()\n",
    "    cols_to_modify_outlier = []\n",
    "\n",
    "    # Manual selection of numeric columns to modify for outliers\n",
    "    potential_outlier_cols = [\n",
    "        \"price\",\n",
    "        \"freight_value\",\n",
    "        \"payment_value\",\n",
    "        \"product_weight_g\",\n",
    "        \"product_length_cm\",\n",
    "        \"product_height_cm\",\n",
    "        \"product_width_cm\",\n",
    "        \"geolocation_lat\",\n",
    "        \"geolocation_lng\",\n",
    "        \"payment_installments\",\n",
    "        \"review_score\",\n",
    "        \"customer_zip_code_prefix\",\n",
    "        \"seller_zip_code_prefix\",\n",
    "    ]\n",
    "\n",
    "    for col in potential_outlier_cols:\n",
    "        if col in numeric_cols:\n",
    "            cols_to_modify_outlier.append(col)\n",
    "\n",
    "    for col in cols_to_modify_outlier:\n",
    "        num_outliers = int(outlier_ratio * len(df_copy))\n",
    "        indices_to_modify = random.sample(df_copy.index.tolist(), num_outliers)\n",
    "\n",
    "        for idx in indices_to_modify:\n",
    "            original_dtype = df_copy[col].dtype\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                # Add over value\n",
    "                over_value = df_copy[col].max() * (1 + random.random()) * 5\n",
    "                if pd.api.types.is_integer_dtype(original_dtype):\n",
    "                    df_copy.loc[idx, col] = int(over_value)\n",
    "                else:\n",
    "                    df_copy.loc[idx, col] = over_value\n",
    "            else:\n",
    "                # Add under value\n",
    "                under_value = df_copy[col].min() * (random.random() * 0.1)\n",
    "                if pd.api.types.is_integer_dtype(original_dtype):\n",
    "                    df_copy.loc[idx, col] = int(under_value)\n",
    "                else:\n",
    "                    df_copy.loc[idx, col] = under_value\n",
    "\n",
    "        print(f\"Number of outliers added in column '{col}': {num_outliers}\")\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for remove random rows data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_randomly(df, num_rows_to_remove=None, percentage_to_remove=None):\n",
    "    \"\"\"\n",
    "    Removes a specified number or percentage of rows randomly from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame from which rows will be removed.\n",
    "        num_rows_to_remove (int, optional): The exact number of rows to remove. Defaults to None.\n",
    "        percentage_to_remove (float, optional): The percentage of rows to remove (0.0 - 1.0). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the specified rows removed.\n",
    "                      Returns the original DataFrame if neither num_rows_to_remove nor\n",
    "                      percentage_to_remove is provided or if the input values are invalid.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    total_rows = len(df_copy)\n",
    "    rows_to_drop = []\n",
    "\n",
    "    if num_rows_to_remove is not None:\n",
    "        if isinstance(num_rows_to_remove, int) and 0 < num_rows_to_remove < total_rows:\n",
    "            rows_to_drop = random.sample(range(total_rows), num_rows_to_remove)\n",
    "        else:\n",
    "            print(\n",
    "                \"Invalid value for num_rows_to_remove. Please provide a positive integer less than the total number of rows.\"\n",
    "            )\n",
    "            return df_copy\n",
    "    elif percentage_to_remove is not None:\n",
    "        if isinstance(percentage_to_remove, float) and 0.0 < percentage_to_remove < 1.0:\n",
    "            num_rows_to_drop = int(percentage_to_remove * total_rows)\n",
    "            rows_to_drop = random.sample(range(total_rows), num_rows_to_drop)\n",
    "        else:\n",
    "            print(\n",
    "                \"Invalid value for percentage_to_remove. Please provide a float between 0.0 and 1.0.\"\n",
    "            )\n",
    "            return df_copy\n",
    "    else:\n",
    "        print(\"Please provide either num_rows_to_remove or percentage_to_remove.\")\n",
    "        return df_copy\n",
    "\n",
    "    if rows_to_drop:\n",
    "        df_copy = df_copy.drop(rows_to_drop).reset_index(drop=True)\n",
    "        print(f\"Removed {len(rows_to_drop)} rows from the DataFrame.\")\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & save new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv(\"../data/customers_dataset.csv\")\n",
    "geolocation_df = pd.read_csv(\"../data/geolocation_dataset.csv\")\n",
    "order_item_df = pd.read_csv(\"../data/order_items_dataset.csv\")\n",
    "order_payments_df = pd.read_csv(\"../data/order_payments_dataset.csv\")\n",
    "order_reviews_df = pd.read_csv(\"../data/order_reviews_dataset.csv\")\n",
    "orders_df = pd.read_csv(\"../data/orders_dataset.csv\")\n",
    "product_category_name_df = pd.read_csv(\"../data/product_category.csv\")\n",
    "products_df = pd.read_csv(\"../data/products_dataset.csv\")\n",
    "seller_df = pd.read_csv(\"../data/sellers_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 19888 rows from the DataFrame.\n",
      "Removed 700114 rows from the DataFrame.\n",
      "Removed 45060 rows from the DataFrame.\n",
      "Removed 41554 rows from the DataFrame.\n",
      "Removed 39689 rows from the DataFrame.\n",
      "Removed 39776 rows from the DataFrame.\n",
      "Removed 13180 rows from the DataFrame.\n",
      "Removed 1238 rows from the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "customer_df_remove_lines = remove_rows_randomly(customer_df, percentage_to_remove=0.2)\n",
    "geolocation_df_remove_lines = remove_rows_randomly(\n",
    "    geolocation_df, percentage_to_remove=0.7\n",
    ")\n",
    "order_item_df_remove_lines = remove_rows_randomly(\n",
    "    order_item_df, percentage_to_remove=0.4\n",
    ")\n",
    "order_payment_df_remove_lines = remove_rows_randomly(\n",
    "    order_payments_df, percentage_to_remove=0.4\n",
    ")\n",
    "order_review_df_remove_lines = remove_rows_randomly(\n",
    "    order_reviews_df, percentage_to_remove=0.4\n",
    ")\n",
    "orders_df_remove_lines = remove_rows_randomly(orders_df, percentage_to_remove=0.4)\n",
    "products_df_remove_lines = remove_rows_randomly(products_df, percentage_to_remove=0.4)\n",
    "seller_df_remove_lines = remove_rows_randomly(seller_df, percentage_to_remove=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values (NaN) added (excluding ID columns): 14319\n",
      "Number of outliers added in column 'customer_zip_code_prefix': 3977\n",
      "Number of missing values (NaN) added (excluding ID columns): 90014\n",
      "Number of outliers added in column 'geolocation_lat': 15002\n",
      "Number of outliers added in column 'geolocation_lng': 15002\n",
      "Number of missing values (NaN) added (excluding ID columns): 16221\n",
      "Number of outliers added in column 'price': 3379\n",
      "Number of outliers added in column 'freight_value': 3379\n",
      "Number of missing values (NaN) added (excluding ID columns): 14959\n",
      "Number of outliers added in column 'payment_value': 3116\n",
      "Number of outliers added in column 'payment_installments': 3116\n",
      "Number of missing values (NaN) added (excluding ID columns): 17860\n",
      "Number of outliers added in column 'review_score': 2976\n",
      "Number of missing values (NaN) added (excluding ID columns): 21479\n",
      "Number of missing values (NaN) added (excluding ID columns): 8\n",
      "Number of missing values (NaN) added (excluding ID columns): 9490\n",
      "Number of outliers added in column 'product_weight_g': 988\n",
      "Number of outliers added in column 'product_length_cm': 988\n",
      "Number of outliers added in column 'product_height_cm': 988\n",
      "Number of outliers added in column 'product_width_cm': 988\n",
      "Number of missing values (NaN) added (excluding ID columns): 334\n",
      "Number of outliers added in column 'seller_zip_code_prefix': 92\n"
     ]
    }
   ],
   "source": [
    "customer_df_dirty = make_data_dirty(customer_df_remove_lines)\n",
    "geolocation_df_dirty = make_data_dirty(geolocation_df_remove_lines)\n",
    "order_item_df_dirty = make_data_dirty(order_item_df_remove_lines)\n",
    "order_payment_df_dirty = make_data_dirty(order_payment_df_remove_lines)\n",
    "order_review_df_dirty = make_data_dirty(order_review_df_remove_lines)\n",
    "orders_df_dirty = make_data_dirty(orders_df_remove_lines)\n",
    "product_category_name_translation_df_dirty = make_data_dirty(product_category_name_df)\n",
    "products_df_dirty = make_data_dirty(products_df_remove_lines)\n",
    "seller_df_dirty = make_data_dirty(seller_df_remove_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'dataset' belum ada, berhasil dibuat.\n",
      "\n",
      "Memulai proses penyimpanan DataFrame...\n",
      "DataFrame 'customer_data_dirty' berhasil disimpan ke 'dataset/customer_data_dirty.csv'\n",
      "DataFrame 'geolocation_dirty' berhasil disimpan ke 'dataset/geolocation_dirty.csv'\n",
      "DataFrame 'order_item_dirty' berhasil disimpan ke 'dataset/order_item_dirty.csv'\n",
      "DataFrame 'order_payment_dirty' berhasil disimpan ke 'dataset/order_payment_dirty.csv'\n",
      "DataFrame 'order_review_dirty' berhasil disimpan ke 'dataset/order_review_dirty.csv'\n",
      "DataFrame 'orders_dataset_dirty' berhasil disimpan ke 'dataset/orders_dataset_dirty.csv'\n",
      "DataFrame 'product_category' berhasil disimpan ke 'dataset/product_category.csv'\n",
      "DataFrame 'products_dirty' berhasil disimpan ke 'dataset/products_dirty.csv'\n",
      "DataFrame 'seller_dirty' berhasil disimpan ke 'dataset/seller_dirty.csv'\n",
      "\n",
      "Proses penyimpanan selesai.\n"
     ]
    }
   ],
   "source": [
    "# List berisi nama DataFrame yang udah dikotori\n",
    "dirty_dfs = {\n",
    "    \"customer_data_dirty\": customer_df_dirty,\n",
    "    \"geolocation_dirty\": geolocation_df_dirty,\n",
    "    \"order_item_dirty\": order_item_df_dirty,\n",
    "    \"order_payment_dirty\": order_payment_df_dirty,\n",
    "    \"order_review_dirty\": order_review_df_dirty,\n",
    "    \"orders_dataset_dirty\": orders_df_dirty,\n",
    "    \"product_category\": product_category_name_translation_df_dirty,\n",
    "    \"products_dirty\": products_df_dirty,\n",
    "    \"seller_dirty\": seller_df_dirty,\n",
    "}\n",
    "\n",
    "target_folder = \"dataset\"\n",
    "# 2. Periksa apakah folder target sudah ada\n",
    "if not os.path.exists(target_folder):\n",
    "    # 3. Jika belum ada, buat foldernya\n",
    "    try:\n",
    "        os.makedirs(target_folder)\n",
    "        print(f\"Folder '{target_folder}' belum ada, berhasil dibuat.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Gagal membuat folder '{target_folder}'. Error: {e}\")\n",
    "        # Anda mungkin ingin menghentikan script di sini jika folder gagal dibuat\n",
    "        # exit()\n",
    "else:\n",
    "    print(f\"Folder '{target_folder}' sudah ada.\")\n",
    "\n",
    "# 4. Lanjutkan loop untuk menyimpan DataFrame ke CSV\n",
    "print(\"\\nMemulai proses penyimpanan DataFrame...\")\n",
    "for name, df in dirty_dfs.items():\n",
    "    # Gunakan os.path.join untuk membuat path file yang valid di berbagai OS\n",
    "    filepath = os.path.join(target_folder, f\"{name}.csv\")\n",
    "\n",
    "    try:\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"DataFrame '{name}' berhasil disimpan ke '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        # Menangani potensi error saat menyimpan file (misal, masalah izin)\n",
    "        print(f\"Gagal menyimpan DataFrame '{name}' ke '{filepath}'. Error: {e}\")\n",
    "\n",
    "print(\"\\nProses penyimpanan selesai.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dirty(df, missing_value_ratio=0.05, outlier_ratio=0.01):\n",
    "    \"\"\"\n",
    "    A function to make the data 'dirty' by adding missing values (NaN)\n",
    "    randomly and adding outliers to some numeric columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be made dirty.\n",
    "        missing_value_ratio (float): The proportion of values to be changed to NaN (0.0 - 1.0).\n",
    "        outlier_ratio (float): The proportion of values to be changed to outliers (0.0 - 1.0).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dirty DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    num_rows, num_cols = df_copy.shape\n",
    "\n",
    "    # 1. Add missing values (NaN) randomly\n",
    "    num_missing = int(missing_value_ratio * num_rows * num_cols)\n",
    "    cells_to_null = random.sample(range(num_rows * num_cols), num_missing)\n",
    "\n",
    "    for idx in cells_to_null:\n",
    "        row_idx = idx // num_cols\n",
    "        col_idx = idx % num_cols\n",
    "        df_copy.iloc[row_idx, col_idx] = np.nan\n",
    "\n",
    "    print(f\"Number of missing values (NaN) added: {num_missing}\")\n",
    "\n",
    "    # 2. Add outliers to some numeric columns (manually selected)\n",
    "    numeric_cols = df_copy.select_dtypes(include=np.number).columns.tolist()\n",
    "    cols_to_modify_outlier = []\n",
    "\n",
    "    # Manual selection of numeric columns to modify for outliers\n",
    "    potential_outlier_cols = [\n",
    "        \"price\",\n",
    "        \"freight_value\",\n",
    "        \"payment_value\",\n",
    "        \"product_weight_g\",\n",
    "        \"product_length_cm\",\n",
    "        \"product_height_cm\",\n",
    "        \"product_width_cm\",\n",
    "        \"geolocation_lat\",\n",
    "        \"geolocation_lng\",\n",
    "        \"payment_installments\",\n",
    "        \"review_score\",\n",
    "        \"order_item_id\",\n",
    "        \"customer_zip_code_prefix\",\n",
    "        \"seller_zip_code_prefix\",\n",
    "    ]\n",
    "\n",
    "    for col in potential_outlier_cols:\n",
    "        if col in numeric_cols:\n",
    "            cols_to_modify_outlier.append(col)\n",
    "\n",
    "    for col in cols_to_modify_outlier:\n",
    "        num_outliers = int(outlier_ratio * len(df_copy))\n",
    "        indices_to_modify = random.sample(df_copy.index.tolist(), num_outliers)\n",
    "\n",
    "        for idx in indices_to_modify:\n",
    "            original_dtype = df_copy[col].dtype\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                # Add over value\n",
    "                over_value = df_copy[col].max() * (1 + random.random()) * 5\n",
    "                if pd.api.types.is_integer_dtype(original_dtype):\n",
    "                    df_copy.loc[idx, col] = int(over_value)\n",
    "                else:\n",
    "                    df_copy.loc[idx, col] = over_value\n",
    "            else:\n",
    "                # Add under value\n",
    "                under_value = df_copy[col].min() * (random.random() * 0.1)\n",
    "                if pd.api.types.is_integer_dtype(original_dtype):\n",
    "                    df_copy.loc[idx, col] = int(under_value)\n",
    "                else:\n",
    "                    df_copy.loc[idx, col] = under_value\n",
    "\n",
    "        print(f\"Number of outliers added in column '{col}': {num_outliers}\")\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv(\"../data/customers_dataset.csv\")\n",
    "geolocation_df = pd.read_csv(\"../data/geolocation_dataset.csv\")\n",
    "order_item_df = pd.read_csv(\"../data/order_items_dataset.csv\")\n",
    "order_payments_df = pd.read_csv(\"../data/order_payments_dataset.csv\")\n",
    "order_reviews_df = pd.read_csv(\"../data/order_reviews_dataset.csv\")\n",
    "orders_df = pd.read_csv(\"../data/orders_dataset.csv\")\n",
    "product_category_name_df = pd.read_csv(\"../data/product_category.csv\")\n",
    "products_df = pd.read_csv(\"../data/products_dataset.csv\")\n",
    "seller_df = pd.read_csv(\"../data/sellers_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df_dirty = make_data_dirty(customer_df)\n",
    "geolocation_df_dirty = make_data_dirty(geolocation_df)\n",
    "order_item_df_dirty = make_data_dirty(order_item_df)\n",
    "order_payment_df_dirty = make_data_dirty(order_payments_df)\n",
    "order_review_df_dirty = make_data_dirty(order_reviews_df)\n",
    "orders_df_dirty = make_data_dirty(orders_df)\n",
    "product_category_name_translation_df_dirty = make_data_dirty(product_category_name_df)\n",
    "products_df_dirty = make_data_dirty(products_df)\n",
    "seller_df_dirty = make_data_dirty(seller_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List berisi nama DataFrame yang udah dikotori\n",
    "dirty_dfs = {\n",
    "    \"customer_data_dirty\": customer_df_dirty,\n",
    "    \"geolocation_dirty\": geolocation_df_dirty,\n",
    "    \"order_item_dirty\": order_payment_df_dirty,\n",
    "    \"order_payment_dirty\": order_payment_df_dirty,\n",
    "    \"order_review_dirty\": order_review_df_dirty,\n",
    "    \"orders_dataset_dirty\": orders_df_dirty,\n",
    "    \"product_category_name_translation_dirty\": product_category_name_translation_df_dirty,\n",
    "    \"products_dirty\": products_df_dirty,\n",
    "    \"seller_dirty\": seller_df_dirty,\n",
    "}\n",
    "\n",
    "# Looping buat save setiap DataFrame ke file CSV\n",
    "for name, df in dirty_dfs.items():\n",
    "    filepath = f\"../dataset/{name}.csv\"\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"DataFrame '{name}' berhasil disimpan ke '{filepath}'\")\n",
    "\n",
    "print(\"\\nSemua data yang udah dikotori udah disimpan!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
